{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "\n",
    "# DATA LOADING\n",
    "def load_unlabeled_images(root_folder, image_size=(224, 224)):\n",
    "   \"\"\"\n",
    "   Loads all images from the 'UnlabeledDataset' directory for self-supervised learning.\n",
    "  \n",
    "   Args:\n",
    "       root_folder (str): Path to the folder containing 'Train' and 'Test' subdirectories.\n",
    "       image_size (tuple): Target size for resizing images.\n",
    "  \n",
    "   Returns:\n",
    "       np.array: Array of loaded and normalized images.\n",
    "   \"\"\"\n",
    "   images_list = []\n",
    "   for split in [\"Train\", \"Test\"]:\n",
    "       split_dir = os.path.join(root_folder, split)\n",
    "       if not os.path.exists(split_dir):\n",
    "           print(f\"Warning: Directory {split_dir} not found. Skipping...\")\n",
    "           continue\n",
    "       for fn in os.listdir(split_dir):\n",
    "           if fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "               img_path = os.path.join(split_dir, fn)\n",
    "               try:\n",
    "                   image = cv2.imread(img_path)\n",
    "                   image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                   image = cv2.resize(image, image_size)\n",
    "                   image = image.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
    "                   images_list.append(image)\n",
    "               except Exception as e:\n",
    "                   print(f\"[Skip] {img_path}: {e}\")\n",
    "   if len(images_list) == 0:\n",
    "       raise ValueError(\"No images were loaded. Please check the dataset structure.\")\n",
    "   return np.stack(images_list, axis=0)\n",
    "\n",
    "\n",
    "def random_augment(image):\n",
    "   \"\"\"\n",
    "   Applies a series of random augmentations for contrastive learning.\n",
    "  \n",
    "   Args:\n",
    "       image (tf.Tensor): Input image tensor.\n",
    "  \n",
    "   Returns:\n",
    "       tf.Tensor: Augmented image.\n",
    "   \"\"\"\n",
    "   image = tf.image.random_flip_left_right(image)\n",
    "   image = tf.image.random_brightness(image, max_delta=0.4)\n",
    "   image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "   image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "   image = tf.image.rot90(image, k=random.randint(0, 3))\n",
    "   image = tf.image.random_crop(image, size=[180, 180, 3])\n",
    "   image = tf.image.resize(image, (224, 224))\n",
    "   return image\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ENCODER EXTRACTION\n",
    "# ------------------------------\n",
    "def extract_encoder(mae_model):\n",
    "   \"\"\"\n",
    "   Extracts the encoder part from a trained MAE teacher model.\n",
    "  \n",
    "   For the new MAEv5 architecture, we select the output of the layer named \"dense\"\n",
    "   (which produces the 512-dimensional latent vector) as the feature representation.\n",
    "  \n",
    "   Args:\n",
    "       mae_model (tf.keras.Model): Pre-trained MAE teacher model.\n",
    "      \n",
    "   Returns:\n",
    "       tf.keras.Model: Encoder model.\n",
    "   \"\"\"\n",
    "   encoder_output = mae_model.get_layer(\"dense\").output  # Use the layer named \"dense\"\n",
    "   encoder = Model(inputs=mae_model.input, outputs=encoder_output)\n",
    "   return encoder\n",
    "\n",
    "\n",
    "# CONTRASTIVE LOSS FUNCTION (NT-Xent)\n",
    "\n",
    "\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "   \"\"\"\n",
    "   Computes the NT-Xent (Normalized Temperature-scaled Cross-Entropy) loss.\n",
    "  \n",
    "   Args:\n",
    "       z_i (tf.Tensor): Feature vector from augmented view 1.\n",
    "       z_j (tf.Tensor): Feature vector from augmented view 2.\n",
    "       temperature (float): Temperature scaling parameter.\n",
    "      \n",
    "   Returns:\n",
    "       tf.Tensor: NT-Xent loss.\n",
    "   \"\"\"\n",
    "   # Reshape to [batch, feature_dim]\n",
    "   z_i = tf.reshape(z_i, [tf.shape(z_i)[0], -1])\n",
    "   z_j = tf.reshape(z_j, [tf.shape(z_j)[0], -1])\n",
    "  \n",
    "   # Normalize features\n",
    "   z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "   z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "  \n",
    "   # Compute cosine similarity matrix scaled by temperature\n",
    "   logits = tf.matmul(z_i, tf.transpose(z_j)) / temperature\n",
    "   labels = tf.range(tf.shape(z_i)[0])  # Each example should match itself\n",
    "  \n",
    "   loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "   return loss\n",
    "\n",
    "\n",
    "# SIMCLR FINE-TUNING (CONTRASTIVE LEARNING)\n",
    "\n",
    "\n",
    "def fine_tune_with_simclr(mae_model_path, root_folder, batch_size=32, epochs=5):\n",
    "   \"\"\"\n",
    "   Fine-tunes the MAE encoder using SimCLR contrastive learning.\n",
    "  \n",
    "   Args:\n",
    "       mae_model_path (str): Path to the pre-trained MAE teacher model file.\n",
    "       root_folder (str): Path to the dataset containing unlabeled images.\n",
    "       batch_size (int): Batch size for training.\n",
    "       epochs (int): Number of training epochs.\n",
    "  \n",
    "   Returns:\n",
    "       tf.keras.Model: The fine-tuned encoder model.\n",
    "   \"\"\"\n",
    "   print(\"Loading pre-trained MAE model...\")\n",
    "   mae_model = tf.keras.models.load_model(mae_model_path)\n",
    "   encoder = extract_encoder(mae_model)\n",
    "   print(\"Encoder successfully extracted!\")\n",
    "  \n",
    "   print(\"Loading unlabeled images for SimCLR training...\")\n",
    "   images = load_unlabeled_images(root_folder)\n",
    "  \n",
    "   # Create a dataset of augmented image pairs\n",
    "   dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "   dataset = dataset.map(lambda x: (random_augment(x), random_augment(x)))\n",
    "   dataset = dataset.batch(batch_size).shuffle(500).prefetch(tf.data.AUTOTUNE)\n",
    "  \n",
    "   optimizer = Adam(learning_rate=0.001)\n",
    "  \n",
    "   print(\"Starting SimCLR fine-tuning...\")\n",
    "   for epoch in range(epochs):\n",
    "       epoch_loss = 0\n",
    "       steps = 0\n",
    "       for step, (x_i, x_j) in enumerate(dataset):\n",
    "           with tf.GradientTape() as tape:\n",
    "               z_i = encoder(x_i, training=True)\n",
    "               z_j = encoder(x_j, training=True)\n",
    "               loss = nt_xent_loss(z_i, z_j)\n",
    "           gradients = tape.gradient(loss, encoder.trainable_variables)\n",
    "           optimizer.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
    "           epoch_loss += tf.reduce_mean(loss).numpy()\n",
    "           steps += 1\n",
    "           if step % 10 == 0:\n",
    "               print(f\"Epoch {epoch+1}/{epochs}, Step {step}, Loss: {tf.reduce_mean(loss).numpy():.4f}\")\n",
    "       avg_loss = epoch_loss / steps\n",
    "       print(f\"Epoch {epoch+1}/{epochs} completed. Avg SimCLR Loss: {avg_loss:.4f}\")\n",
    "  \n",
    "   # Save the fine-tuned encoder using the native Keras format (avoid HDF5 issues)\n",
    "   encoder.save(\"ssl_teacher_model.keras\")\n",
    "   print(\"Fine-tuning complete! Encoder saved as 'ssl_teacher_model.keras'.\")\n",
    "  \n",
    "   return encoder\n",
    "\n",
    "\n",
    "# RUN FINE-TUNING\n",
    "if __name__ == \"__main__\":\n",
    "   root_folder = \"UnlabeledDataset\"  # Path to unlabeled images\n",
    "   mae_model_path = \"mae_teacher_model_improved.h5\"  # Path to improved MAE teacher model\n",
    "   ssl_teacher = fine_tune_with_simclr(mae_model_path, root_folder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
