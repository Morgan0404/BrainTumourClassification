{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIG & PATHS\n",
    "# ------------------------------\n",
    "labeled_data_path = \"/Users/morgan/Desktop/BT-ClassificationDissCode/combineddataset/Training\"\n",
    "teacher_model_path = \"ssl_teacher_model.keras\"\n",
    "output_model_path = \"teacher_classifier_modelv5.keras\"\n",
    "num_classes = 3\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# DATA LOADING\n",
    "# ------------------------------\n",
    "def load_labeled_dataset(root_folder, image_size=(224, 224)):\n",
    "   images_list, labels_list = [], []\n",
    "   class_mapping = {\"glioma\": 0, \"meningioma\": 1, \"pituitary\": 2}\n",
    "\n",
    "\n",
    "   for class_name, label in class_mapping.items():\n",
    "       label_dir = os.path.join(root_folder, class_name)\n",
    "       if not os.path.isdir(label_dir):\n",
    "           print(f\"Warning: Directory not found -> {label_dir}\")\n",
    "           continue\n",
    "\n",
    "\n",
    "       for fn in os.listdir(label_dir):\n",
    "           if fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "               img_path = os.path.join(label_dir, fn)\n",
    "               try:\n",
    "                   image = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
    "                   image = tf.keras.preprocessing.image.img_to_array(image) / 255.0\n",
    "                   # Apply expanded data augmentation\n",
    "                   image = tf.image.random_flip_left_right(image)\n",
    "                   image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "                   image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "                   image = tf.image.rot90(image, k=tf.random.uniform(shape=(), minval=0, maxval=4, dtype=tf.int32))\n",
    "                   # Simulate random zoom with resize and random scaling\n",
    "                   scale = tf.random.uniform(shape=(), minval=0.9, maxval=1.1, dtype=tf.float32)\n",
    "                   new_size = (int(image_size[0] * scale), int(image_size[1] * scale))\n",
    "                   image = tf.image.resize(image, new_size)\n",
    "                   image = tf.image.resize(image, image_size)  # Resize back to 224x224\n",
    "                   images_list.append(image.numpy())  # Convert tensor back to numpy for stacking\n",
    "                   labels_list.append(label)\n",
    "               except Exception as e:\n",
    "                   print(f\"⚠️ [Skip] {img_path}: {e}\")\n",
    "\n",
    "\n",
    "   if len(images_list) == 0:\n",
    "       raise ValueError(\"No labeled images found! Check dataset structure.\")\n",
    "\n",
    "\n",
    "   return np.array(images_list), np.array(labels_list)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# BUILD CLASSIFIER HEAD\n",
    "# ------------------------------\n",
    "def build_teacher_classifier(teacher_model_path, num_classes):\n",
    "   print(\"Loading fine-tuned SSL teacher model...\")\n",
    "   teacher_model = tf.keras.models.load_model(teacher_model_path)\n",
    "\n",
    "\n",
    "   # If teacher output is more than 2D, pool it\n",
    "   last_layer = teacher_model.output\n",
    "   if len(last_layer.shape) > 2:\n",
    "       last_layer = GlobalAveragePooling2D(name=\"global_avg_pool\")(last_layer)\n",
    "\n",
    "\n",
    "   # Add a dense layer with L2 regularization, dropout, batch normalization, and final softmax\n",
    "   x = Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "             name=\"dense_classifier_1\")(last_layer)\n",
    "   x = BatchNormalization()(x)  # Added for stability\n",
    "   x = Dropout(0.7, name=\"dropout_classifier\")(x)  # Increased dropout to 0.7\n",
    "   classifier_output = Dense(num_classes, activation=\"softmax\", name=\"classification_head\")(x)\n",
    "\n",
    "\n",
    "   classifier_model = Model(inputs=teacher_model.input, outputs=classifier_output)\n",
    "   print(\"Teacher model updated with classification head!\")\n",
    "   return classifier_model\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TRAIN CLASSIFICATION HEAD\n",
    "# ------------------------------\n",
    "def train_classifier(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=20):\n",
    "   # One-hot encode labels\n",
    "   y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "   y_val_one_hot = tf.keras.utils.to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "\n",
    "   # Class weights\n",
    "   class_weights = compute_class_weight(\n",
    "       class_weight=\"balanced\",\n",
    "       classes=np.unique(y_train),\n",
    "       y=y_train\n",
    "   )\n",
    "   class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "\n",
    "   # Compile with cosine decay learning rate\n",
    "   lr_schedule = tf.keras.optimizers.schedules.CosineDecay(1e-4, epochs * len(X_train) // batch_size)\n",
    "   model.compile(optimizer=Adam(learning_rate=lr_schedule),\n",
    "                 loss=\"categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "   # Callbacks\n",
    "   early_stopping = EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
    "   reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
    "   checkpoint = ModelCheckpoint(\"best_teacher_classifier.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "\n",
    "\n",
    "   print(\"Starting training...\")\n",
    "   history = model.fit(X_train, y_train_one_hot,\n",
    "                       validation_data=(X_val, y_val_one_hot),\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       class_weight=class_weight_dict,\n",
    "                       callbacks=[early_stopping, reduce_lr, checkpoint])\n",
    "\n",
    "\n",
    "   return model, history\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "   print(\"Loading labeled dataset...\")\n",
    "   X, y = load_labeled_dataset(labeled_data_path)\n",
    "\n",
    "\n",
    "   print(\"Splitting dataset into training and validation sets...\")\n",
    "   X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "   print(\"Building classifier model on top of teacher encoder...\")\n",
    "   teacher_classifier = build_teacher_classifier(teacher_model_path, num_classes)\n",
    "\n",
    "\n",
    "   print(\"Training classifier on labeled MRI dataset...\")\n",
    "   trained_classifier, history = train_classifier(teacher_classifier, X_train, y_train, X_val, y_val, epochs=epochs)\n",
    "\n",
    "\n",
    "   trained_classifier.save(output_model_path)\n",
    "   print(f\"Trained teacher classifier saved as '{output_model_path}'\")\n",
    "\n",
    "\n",
    "   def plot_training_history(history):\n",
    "       plt.figure(figsize=(12, 5))\n",
    "       plt.subplot(1, 2, 1)\n",
    "       plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "       plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "       plt.title(\"Training & Validation Accuracy\")\n",
    "       plt.legend()\n",
    "       plt.subplot(1, 2, 2)\n",
    "       plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "       plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "       plt.title(\"Training & Validation Loss\")\n",
    "       plt.legend()\n",
    "       plt.show()\n",
    "\n",
    "\n",
    "   print(\"Plotting training history...\")\n",
    "   plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
